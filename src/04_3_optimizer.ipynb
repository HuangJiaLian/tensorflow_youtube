{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Iter 0, Testing Accuracy 0.9139 , Training Accuracy 0.91005456\n",
      "Iter 1, Testing Accuracy 0.9294 , Training Accuracy 0.9285273\n",
      "Iter 2, Testing Accuracy 0.9348 , Training Accuracy 0.9369818\n",
      "Iter 3, Testing Accuracy 0.9397 , Training Accuracy 0.94225454\n",
      "Iter 4, Testing Accuracy 0.944 , Training Accuracy 0.94698185\n",
      "Iter 5, Testing Accuracy 0.945 , Training Accuracy 0.9494182\n",
      "Iter 6, Testing Accuracy 0.9491 , Training Accuracy 0.9532545\n",
      "Iter 7, Testing Accuracy 0.9522 , Training Accuracy 0.9558909\n",
      "Iter 8, Testing Accuracy 0.9479 , Training Accuracy 0.95494545\n",
      "Iter 9, Testing Accuracy 0.953 , Training Accuracy 0.9596909\n",
      "Iter 10, Testing Accuracy 0.9558 , Training Accuracy 0.96134543\n",
      "Iter 11, Testing Accuracy 0.9557 , Training Accuracy 0.9605455\n",
      "Iter 12, Testing Accuracy 0.9561 , Training Accuracy 0.96425456\n",
      "Iter 13, Testing Accuracy 0.9526 , Training Accuracy 0.96032727\n",
      "Iter 14, Testing Accuracy 0.9608 , Training Accuracy 0.96656364\n",
      "Iter 15, Testing Accuracy 0.9596 , Training Accuracy 0.96681815\n",
      "Iter 16, Testing Accuracy 0.9591 , Training Accuracy 0.96672726\n",
      "Iter 17, Testing Accuracy 0.9616 , Training Accuracy 0.96925455\n",
      "Iter 18, Testing Accuracy 0.9611 , Training Accuracy 0.96903634\n",
      "Iter 19, Testing Accuracy 0.9615 , Training Accuracy 0.9706\n",
      "Iter 20, Testing Accuracy 0.9585 , Training Accuracy 0.96729094\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "LAYER1_NODE = 200\n",
    "LAYER2_NODE = 200\n",
    "LAYER3_NODE = 100\n",
    "LAYER4_NODE = 100\n",
    "# Load data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data\", one_hot=True)\n",
    "\n",
    "batch_size = 200\n",
    "# Number of batch \n",
    "n_batch = mnist.train.num_examples // batch_size\n",
    "\n",
    "# Define placeholder\n",
    "x = tf.placeholder(tf.float32,[None, 784])\n",
    "y = tf.placeholder(tf.float32,[None, 10]) # Lable\n",
    "keep_prob = tf.placeholder(tf.float32) # How much percentage of node is working\n",
    "\n",
    "\n",
    "# Build Network\n",
    "# # W1 = tf.Variable(tf.zeros([784,10]))\n",
    "# W1 = tf.Variable(tf.truncated_normal([784,10], stddev=0.1))\n",
    "# # b1 = tf.Variable(tf.zeros([10]))\n",
    "# b1 = tf.Variable(tf.zeros([10]) + 0.1)\n",
    "# prediction = tf.nn.softmax(tf.matmul(x,W1)+b1)\n",
    "\n",
    "W1 = tf.Variable(tf.truncated_normal([784,LAYER1_NODE],stddev=0.1))\n",
    "b1 = tf.Variable(tf.zeros([LAYER1_NODE]) + 0.1)\n",
    "L1 = tf.nn.tanh(tf.matmul(x,W1) + b1)\n",
    "L1_drop = tf.nn.dropout(L1, keep_prob)\n",
    "\n",
    "W2 = tf.Variable(tf.truncated_normal([LAYER1_NODE,LAYER2_NODE],stddev=0.1))\n",
    "b2 = tf.Variable(tf.zeros([LAYER2_NODE]) + 0.1)\n",
    "L2 = tf.nn.tanh(tf.matmul(L1_drop,W2) + b2)\n",
    "L2_drop = tf.nn.dropout(L2, keep_prob)\n",
    "\n",
    "W3 = tf.Variable(tf.truncated_normal([LAYER2_NODE,LAYER3_NODE],stddev=0.1))\n",
    "b3 = tf.Variable(tf.zeros([LAYER3_NODE]) + 0.1)\n",
    "L3 = tf.nn.tanh(tf.matmul(L2_drop,W3) + b3)\n",
    "L3_drop = tf.nn.dropout(L3, keep_prob)\n",
    "\n",
    "\n",
    "W4 = tf.Variable(tf.truncated_normal([LAYER3_NODE,10],stddev=0.1))\n",
    "b4 = tf.Variable(tf.zeros([10]) + 0.1)\n",
    "prediction = tf.nn.softmax(tf.matmul(L3_drop,W4)+b4)\n",
    "\n",
    "\n",
    "# quadratic cost\n",
    "# loss = tf.reduce_mean(tf.square(y-prediction))\n",
    "# cross entropy loss function \n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=prediction))\n",
    "\n",
    "# train_step = tf.train.GradientDescentOptimizer(0.2).minimize(loss)\n",
    "train_step = tf.train.AdamOptimizer(1e-3).minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "# correct_predictionï¼šA boolean list\n",
    "# argmax: return the index of max value \n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(prediction,1))\n",
    "# Get accuracy rate: \n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(21):\n",
    "        for batch in range(n_batch):\n",
    "            # Store image in batch_xs, lable in batch_ys\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            sess.run(train_step, feed_dict={x:batch_xs, y:batch_ys, keep_prob:0.7})\n",
    "        test_acc = sess.run(accuracy, feed_dict={x:mnist.test.images,y:mnist.test.labels, keep_prob:0.7})\n",
    "        train_acc = sess.run(accuracy, feed_dict={x:mnist.train.images,y:mnist.train.labels, keep_prob:0.7})\n",
    "        print(\"Iter \" + str(epoch) + \", Testing Accuracy \" + str(test_acc),\", Training Accuracy \" + str(train_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mooc)",
   "language": "python",
   "name": "python_mooc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
